{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9adb7bd1",
   "metadata": {},
   "source": [
    "# StyleTTS 2 Stage 1 Inference (for korean model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6108384d",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96e173bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da84c60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a3ddcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load packages\n",
    "import time\n",
    "import random\n",
    "import yaml\n",
    "from munch import Munch\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import librosa\n",
    "\n",
    "from models import *\n",
    "from utils import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00ee05e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "\n",
    "to_mel = torchaudio.transforms.MelSpectrogram(\n",
    "    n_mels=80, n_fft=2048, win_length=1200, hop_length=300)\n",
    "mean, std = -4, 4\n",
    "\n",
    "def length_to_mask(lengths):\n",
    "    mask = torch.arange(lengths.max()).unsqueeze(0).expand(lengths.shape[0], -1).type_as(lengths)\n",
    "    mask = torch.gt(mask+1, lengths.unsqueeze(1))\n",
    "    return mask\n",
    "\n",
    "def preprocess(wave):\n",
    "    wave_tensor = torch.from_numpy(wave).float()\n",
    "    mel_tensor = to_mel(wave_tensor)\n",
    "    mel_tensor = (torch.log(1e-5 + mel_tensor.unsqueeze(0)) - mean) / std\n",
    "    return mel_tensor\n",
    "\n",
    "def compute_style(path):\n",
    "    audio, sr = librosa.load(path, sr=24000)\n",
    "    # audio, index = librosa.effects.trim(audio, top_db=30)\n",
    "    # if sr != 24000:\n",
    "    #     audio = librosa.resample(audio, sr, 24000)\n",
    "    mel_tensor = preprocess(audio).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ref_s = model.style_encoder(mel_tensor.unsqueeze(1))\n",
    "        ref_p = model.predictor_encoder(mel_tensor.unsqueeze(1))\n",
    "\n",
    "    return torch.cat([ref_s, ref_p], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdc04c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9cecbe",
   "metadata": {},
   "source": [
    "### Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e7b644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = yaml.safe_load(open(\"Models/LibriTTS_vocos/config_libritts_vocos.yml\"))\n",
    "config = yaml.safe_load(open(\"/data/ckpts/stts2/LibriTTS_vocos/config_libritts_vocos.yml\"))\n",
    "\n",
    "# load pretrained ASR model\n",
    "ASR_config = config.get('ASR_config', False)\n",
    "ASR_path = config.get('ASR_path', False)\n",
    "text_aligner = load_ASR_models(ASR_path, ASR_config)\n",
    "\n",
    "# load pretrained F0 model\n",
    "F0_path = config.get('F0_path', False)\n",
    "pitch_extractor = load_F0_models(F0_path)\n",
    "\n",
    "# load BERT model\n",
    "from Utils.PLBERT.util import load_plbert\n",
    "BERT_path = config.get('PLBERT_dir', False)\n",
    "plbert = load_plbert(BERT_path)\n",
    "\n",
    "model_params = recursive_munch(config['model_params'])\n",
    "model = build_model_no_bert(model_params, text_aligner, pitch_extractor)\n",
    "# If you want to use BERT, you can use the following code\n",
    "# model = build_model(model_params, text_aligner, pitch_extractor, plbert)\n",
    "\n",
    "_ = [model[key].eval() for key in model]\n",
    "_ = [model[key].to(device) for key in model]\n",
    "\n",
    "# params_whole = torch.load(\"Models/LibriTTS_vocos/epoch_2nd_00029.pth\", map_location='cpu')\n",
    "params_whole = torch.load(\"/data/ckpts/stts2/LibriTTS_vocos/epoch_2nd_00029.pth\", map_location='cpu')\n",
    "\n",
    "params = params_whole['net']\n",
    "ignore_modules = []\n",
    "\n",
    "for key in model:\n",
    "    if key in params and key not in ignore_modules:\n",
    "        print('%s loaded' % key)\n",
    "        try:\n",
    "            model[key].load_state_dict(params[key], strict=True)\n",
    "        except:\n",
    "            from collections import OrderedDict\n",
    "            state_dict = params[key]\n",
    "            new_state_dict = OrderedDict()\n",
    "            print(f'{key} key lenghth: {len(model[key].state_dict().keys())}, state_dict length: {len(state_dict.keys())}')\n",
    "            for (k_m, v_m), (k_c, v_c) in zip(model[key].state_dict().items(), state_dict.items()):\n",
    "                new_state_dict[k_m] = v_c\n",
    "            model[key].load_state_dict(new_state_dict, strict=True)\n",
    "_ = [model[key].eval() for key in model]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27851916",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Modules.diffusion.sampler import DiffusionSampler, ADPM2Sampler, KarrasSchedule\n",
    "\n",
    "sampler = DiffusionSampler(\n",
    "    model.diffusion.diffusion,\n",
    "    sampler=ADPM2Sampler(),\n",
    "    sigma_schedule=KarrasSchedule(sigma_min=0.0001, sigma_max=3.0, rho=9.0), # empirical parameters\n",
    "    clamp=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b803110e",
   "metadata": {},
   "source": [
    "### Synthesize speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86e668e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import phonemizer\n",
    "\n",
    "global_phonemizer = phonemizer.backend.EspeakBackend(language='en-us', preserve_punctuation=True,  with_stress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "993afea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from symbols_en import symbols\n",
    "# If you want to use multi lingual vocabs, you can use the following code\n",
    "# from symbols import symbols\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "dicts = {}\n",
    "for i in range(len((symbols))):\n",
    "    dicts[symbols[i]] = i\n",
    "\n",
    "class TextCleaner:\n",
    "    def __init__(self, dummy=None):\n",
    "        self.word_index_dictionary = dicts\n",
    "    def __call__(self, text, cleaned=True):\n",
    "        indexes = []\n",
    "        if not cleaned:\n",
    "            ps = global_phonemizer.phonemize([text])\n",
    "            ps = word_tokenize(ps[0])\n",
    "            ps = ' '.join(ps)\n",
    "        else:\n",
    "            ps = text\n",
    "\n",
    "        for char in ps:\n",
    "            try:\n",
    "                indexes.append(self.word_index_dictionary[char])\n",
    "            except KeyError:\n",
    "                print(f\"Unknown character: {char}\")\n",
    "                print(text)\n",
    "        return indexes\n",
    "\n",
    "textclenaer = TextCleaner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2979f7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def inference(text, ref_s, alpha = 0.3, beta = 0.7, diffusion_steps=5, embedding_scale=1, is_cleaned=False):\n",
    "    text = text.strip()\n",
    "    tokens = textclenaer(text, cleaned=is_cleaned)\n",
    "    tokens.insert(0, 0)\n",
    "    tokens.append(0)\n",
    "    # print(f'Tokens:', tokens)\n",
    "    print(f'The length of tokens is:', len(tokens))\n",
    "    tokens = torch.LongTensor(tokens).to(device).unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        input_lengths = torch.LongTensor([tokens.shape[-1]]).to(device)\n",
    "        text_mask = length_to_mask(input_lengths).to(device)\n",
    "\n",
    "        t_en = model.text_encoder(tokens, input_lengths, text_mask)\n",
    "        d_en = model.prosodic_text_encoder(tokens, input_lengths, text_mask)\n",
    "        d_en_dur = d_en.transpose(-1, -2)\n",
    "\n",
    "\n",
    "        s_pred = sampler(noise = torch.randn((1, 256)).unsqueeze(1).to(device), \n",
    "                                          embedding=d_en_dur,\n",
    "                                          embedding_scale=embedding_scale,\n",
    "                                            features=ref_s, # reference from the same speaker as the embedding\n",
    "                                             num_steps=diffusion_steps).squeeze(1)\n",
    "\n",
    "\n",
    "        s = s_pred[:, 128:]\n",
    "        ref = s_pred[:, :128]\n",
    "\n",
    "        ref = alpha * ref + (1 - alpha) * ref_s[:, :128]\n",
    "\n",
    "        s = beta * s + (1 - beta) * ref_s[:, 128:]\n",
    "        d = model.predictor.text_encoder(d_en, \n",
    "                                        s, input_lengths, text_mask)\n",
    "        x, _ = model.predictor.lstm(d)  \n",
    "\n",
    "        duration = model.predictor.duration_proj(x)\n",
    "        duration = torch.sigmoid(duration).sum(axis=-1)\n",
    "        pred_dur = torch.round(duration.squeeze()).clamp(min=1)\n",
    "\n",
    "        pred_aln_trg = torch.zeros(input_lengths, int(pred_dur.sum().data))\n",
    "        c_frame = 0\n",
    "        for i in range(pred_aln_trg.size(0)):\n",
    "            pred_aln_trg[i, c_frame:c_frame + int(pred_dur[i].data)] = 1\n",
    "            c_frame += int(pred_dur[i].data)\n",
    "\n",
    "        # encode prosody\n",
    "        en = (d.transpose(-1, -2) @ pred_aln_trg.unsqueeze(0).to(device))\n",
    "        # if model_params.decoder.type == \"hifigan\":\n",
    "        if True:\n",
    "            asr_new = torch.zeros_like(en)\n",
    "            asr_new[:, :, 0] = en[:, :, 0]\n",
    "            asr_new[:, :, 1:] = en[:, :, 0:-1]\n",
    "            en = asr_new\n",
    "\n",
    "        F0_pred, N_pred = model.predictor.F0Ntrain(en, s)\n",
    "\n",
    "        asr = (t_en @ pred_aln_trg.unsqueeze(0).to(device))\n",
    "        # if model_params.decoder.type == \"hifigan\":\n",
    "        if True:\n",
    "            asr_new = torch.zeros_like(asr)\n",
    "            asr_new[:, :, 0] = asr[:, :, 0]\n",
    "            asr_new[:, :, 1:] = asr[:, :, 0:-1]\n",
    "            asr = asr_new\n",
    "\n",
    "        start = time.time()\n",
    "        out = model.decoder(asr, \n",
    "                                F0_pred, N_pred, ref.squeeze().unsqueeze(0))\n",
    "        end = time.time()\n",
    "        print(f\"model.decoder time:\", end-start)\n",
    "        \n",
    "        # plot one s2s attention\n",
    "\n",
    "        # plt.figure(figsize=(10, 5))\n",
    "        # plt.imshow(pred_aln_trg.squeeze().cpu().numpy(), aspect='auto', origin='lower')\n",
    "        # plt.colorbar()\n",
    "        # plt.show()\n",
    "        \n",
    "    return out.squeeze().cpu().numpy()[..., :-50] # weird pulse at the end of the model, need to be fixed later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cace9787",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import soundfile as sf\n",
    "import IPython.display as ipd\n",
    "\n",
    "# lines = [\n",
    "#     \"LibriTTS/train-clean-360/100/121669/100_121669_000031_000001.wav|hiː wʌz vˈɛɹi ˈæŋɡɹi, ˌɪndˈiːd, fɚðə pˈɪɡ wʌzɐ ɡɹˈeɪt pˈɛt, ænd hiː hæd wˈɔntᵻd tə kˈiːp ɪt tˈɪl ɪt ɡɹˈuː vˈɛɹi bˈɪɡ.|1079\",\n",
    "#     \"LibriTTS/train-clean-360/100/121669/100_121669_000032_000000.wav|sˌoʊ hiː pˌʊt ˌɔn hɪz kˈoʊt ænd bˈʌkəld ɐ stɹˈæp ɚɹˈaʊnd hɪz wˈeɪst, ænd wɛnt dˌaʊn tə ðə vˈɪlɪdʒ tə sˈiː ɪf hiː kʊd fˈaɪnd ˈaʊt hˌuː hæd stˈoʊlən hɪz pˈɪɡ.|1079\",\n",
    "#     \"LibriTTS/train-clean-360/100/121669/100_121669_000033_000000.wav|ˌʌp ænd dˌaʊn ðə stɹˈiːt hiː wˈɛnt, ænd ɪn ænd ˈaʊt ðə lˈeɪnz, bˌʌt nˈoʊ tɹˈeɪsᵻz ʌvðə pˈɪɡ kʊd hiː fˈaɪnd ˈɛnɪwˌɛɹ.|1079\",\n",
    "#     \"LibriTTS/train-clean-360/100/121669/100_121669_000033_000001.wav|ænd ðæt wʌz nˈoʊ ɡɹˈeɪt wˈʌndɚ, fɚðə pˈɪɡ wʌz ˈiːʔn̩ baɪ ðæt tˈaɪm ænd ɪts bˈoʊnz pˈɪkt klˈiːn.|1079\",\n",
    "# ]\n",
    "# is_cleaned = True\n",
    "\n",
    "lines = [\n",
    "    \"LibriTTS/train-clean-360/100/121669/100_121669_000031_000001.wav|The quick brown fox jumps over the lazy dog.|1079\",\n",
    "    \"LibriTTS/train-clean-360/100/121669/100_121669_000032_000000.wav|She sells seashells by the seashore|1079\",\n",
    "    \"LibriTTS/train-clean-360/100/121669/100_121669_000033_000000.wav|Peter Piper picked a peck of pickled peppers.|1079\",\n",
    "    \"LibriTTS/train-clean-360/100/121669/100_121669_000033_000001.wav|How much wood would a woodchuck chuck if a woodchuck could chuck wood?|1079\",\n",
    "]\n",
    "is_cleaned = False\n",
    "\n",
    "silence = torch.zeros(24000 // 2).numpy()\n",
    "\n",
    "download_dir_name = 'Outputs/libritts_vocos_stage2'\n",
    "\n",
    "# wav_dir = 'Your wav directory'\n",
    "wav_dir = '/data/LibriTTS/'\n",
    "\n",
    "for i, lines in enumerate(lines):\n",
    "    wavs = []\n",
    "    wav_path, text, _ = lines.split('|')\n",
    "    print('Text:', text)\n",
    "    language = text[1:3]\n",
    "    ref_wav_path = os.path.join(wav_dir, wav_path)\n",
    "    ref_s = compute_style(ref_wav_path)\n",
    "    wav = inference(text, ref_s, alpha=0.3, beta=0.7, diffusion_steps=15, is_cleaned=is_cleaned)\n",
    "\n",
    "    ref = librosa.load(ref_wav_path, sr=24000)[0]\n",
    "    wavs.append(ref)\n",
    "    wavs.append(silence)\n",
    "    wavs.append(wav)\n",
    "\n",
    "    res = np.concatenate(wavs, axis=0)\n",
    "    \n",
    "    filenpath = os.path.join(download_dir_name, os.path.basename(wav_path))\n",
    "    filenpath = filenpath.replace('.wav', f'_{i}.wav')\n",
    "    os.makedirs(os.path.dirname(filenpath), exist_ok=True)\n",
    "\n",
    "    # Play the Reference, Silence, and Synthesized\n",
    "    sf.write(filenpath, res, 24000)\n",
    "    # shutil.copy(ref_wav_path, download_dir_name)\n",
    "\n",
    "    # print('Synthesized:', i)\n",
    "    # display(ipd.Audio(wav, rate=24000, normalize=False))\n",
    "\n",
    "    # print('Reference:')\n",
    "    # display(ipd.Audio(ref_wav_path, rate=24000, normalize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d76eaf8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stts2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
