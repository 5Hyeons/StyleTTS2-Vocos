{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9adb7bd1",
   "metadata": {},
   "source": [
    "# StyleTTS 2 Stage 1 Inference (for korean model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6108384d",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96e173bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da84c60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a3ddcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load packages\n",
    "import time\n",
    "import random\n",
    "import yaml\n",
    "from munch import Munch\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import librosa\n",
    "\n",
    "from models import *\n",
    "from utils import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00ee05e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "\n",
    "to_mel = torchaudio.transforms.MelSpectrogram(\n",
    "    n_mels=80, n_fft=2048, win_length=1200, hop_length=300)\n",
    "mean, std = -4, 4\n",
    "\n",
    "def length_to_mask(lengths):\n",
    "    mask = torch.arange(lengths.max()).unsqueeze(0).expand(lengths.shape[0], -1).type_as(lengths)\n",
    "    mask = torch.gt(mask+1, lengths.unsqueeze(1))\n",
    "    return mask\n",
    "\n",
    "def preprocess(path):\n",
    "    wave, sr = librosa.load(path, sr=24000)\n",
    "    # wave, index = librosa.effects.trim(wave, top_db=30)\n",
    "    if sr != 24000:\n",
    "        wave = librosa.resample(wave, sr, 24000)\n",
    "    wave = np.concatenate([np.zeros([5000]), wave, np.zeros([5000])], axis=0)\n",
    "\n",
    "    # display(ipd.Audio(wave, rate=sr, normalize=False))\n",
    "\n",
    "    wave_tensor = torch.from_numpy(wave).float()\n",
    "    mel_tensor = to_mel(wave_tensor)\n",
    "    mel_tensor = (torch.log(1e-5 + mel_tensor.unsqueeze(0)) - mean) / std\n",
    "    return mel_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdc04c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9cecbe",
   "metadata": {},
   "source": [
    "### Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e7b644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = yaml.safe_load(open(\"Models/LibriTTS_vocos/config_libritts_vocos.yml\"))\n",
    "config = yaml.safe_load(open(\"/data/ckpts/stts2/LibriTTS_vocos/config_libritts_vocos.yml\"))\n",
    "\n",
    "# load pretrained ASR model\n",
    "ASR_config = config.get('ASR_config', False)\n",
    "ASR_path = config.get('ASR_path', False)\n",
    "text_aligner = load_ASR_models(ASR_path, ASR_config)\n",
    "\n",
    "# load pretrained F0 model\n",
    "F0_path = config.get('F0_path', False)\n",
    "pitch_extractor = load_F0_models(F0_path)\n",
    "\n",
    "# load BERT model\n",
    "from Utils.PLBERT.util import load_plbert\n",
    "BERT_path = config.get('PLBERT_dir', False)\n",
    "plbert = load_plbert(BERT_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc18cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = recursive_munch(config['model_params'])\n",
    "# model = build_model_no_bert(model_params, text_aligner, pitch_extractor)\n",
    "model = build_model(model_params, text_aligner, pitch_extractor, plbert)\n",
    "_ = [model[key].eval() for key in model]\n",
    "_ = [model[key].to(device) for key in model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64529d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load first stage checkpoint path\n",
    "# params_whole = torch.load(\"Models/LibriTTS_vocos/LibriTTS_vocos_first_stage.pth\", map_location='cpu')\n",
    "params_whole = torch.load(\"/data/ckpts/stts2/LibriTTS_vocos/LibriTTS_vocos_first_stage.pth\", map_location='cpu')\n",
    "\n",
    "params = params_whole['net']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895d9706",
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_modules = ['diffusion', 'wd', 'bert', 'bert_encoder',]\n",
    "\n",
    "for key in model:\n",
    "    if key in params and key not in ignore_modules:\n",
    "        print('%s loaded' % key)\n",
    "        try:\n",
    "            model[key].load_state_dict(params[key], strict=True)\n",
    "        except:\n",
    "            from collections import OrderedDict\n",
    "            state_dict = params[key]\n",
    "            new_state_dict = OrderedDict()\n",
    "            print(f'{key} key lenghth: {len(model[key].state_dict().keys())}, state_dict length: {len(state_dict.keys())}')\n",
    "            for (k_m, v_m), (k_c, v_c) in zip(model[key].state_dict().items(), state_dict.items()):\n",
    "                new_state_dict[k_m] = v_c\n",
    "            model[key].load_state_dict(new_state_dict, strict=True)\n",
    "_ = [model[key].eval() for key in model]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b803110e",
   "metadata": {},
   "source": [
    "### Synthesize speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "993afea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from symbols_en import symbols\n",
    "# If you want to use multi lingual vocabs\n",
    "# from symbols import symbols\n",
    "\n",
    "dicts = {}\n",
    "for i in range(len((symbols))):\n",
    "    dicts[symbols[i]] = i\n",
    "\n",
    "class TextCleaner:\n",
    "    def __init__(self, dummy=None):\n",
    "        self.word_index_dictionary = dicts\n",
    "    def __call__(self, text, cleaned=True):\n",
    "        indexes = []\n",
    "        for char in text:\n",
    "            try:\n",
    "                indexes.append(self.word_index_dictionary[char])\n",
    "            except KeyError:\n",
    "                print(f\"Unknown character: {char}\")\n",
    "                print(text)\n",
    "        return indexes\n",
    "\n",
    "textclenaer = TextCleaner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca57469c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# in case not distributed\n",
    "try:\n",
    "    n_down = model.text_aligner.module.n_down\n",
    "except:\n",
    "    n_down = model.text_aligner.n_down\n",
    "    \n",
    "\n",
    "def inference(text, ref_wav_path):\n",
    "    # text = text.strip()\n",
    "    tokens = textclenaer(text, cleaned=True)\n",
    "    tokens.insert(0, 0)\n",
    "    tokens.append(0)\n",
    "    tokens = torch.LongTensor(tokens).to(device).unsqueeze(0)\n",
    "    \n",
    "    mels = preprocess(ref_wav_path).to(device)\n",
    "    mels = mels[:, :, :mels.size(-1) - 1]\n",
    "    mel_input_length = torch.zeros(1).long().to(device)\n",
    "    mel_input_length[0] = mels.size(-1)\n",
    "    print(mel_input_length)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        input_lengths = torch.LongTensor([tokens.shape[-1]]).to(device)\n",
    "        mel_input_length = mel_input_length // (2 ** n_down)\n",
    "        print(mel_input_length)\n",
    "        mels = mels[..., :mel_input_length * (2 ** n_down)]\n",
    "        mask = length_to_mask(mel_input_length).to('cuda')\n",
    "\n",
    "        ppgs, s2s_pred, s2s_attn = model.text_aligner(mels, mask, tokens)\n",
    "\n",
    "        _, amax_s2s = torch.max(s2s_pred, dim=2)\n",
    "\n",
    "        print(''.join([symbols[s1.item()] for s1 in amax_s2s[0]]))\n",
    "        print(''.join([symbols[s2.item()] for s2 in tokens[0]]))\n",
    "\n",
    "        s2s_attn = s2s_attn.transpose(-1, -2)\n",
    "        s2s_attn = s2s_attn[..., 1:]\n",
    "        s2s_attn = s2s_attn.transpose(-1, -2)\n",
    "\n",
    "        text_mask = length_to_mask(input_lengths).to(tokens.device)\n",
    "        attn_mask = (~mask).unsqueeze(-1).expand(mask.shape[0], mask.shape[1], text_mask.shape[-1]).float().transpose(-1, -2)\n",
    "        attn_mask = attn_mask.float() * (~text_mask).unsqueeze(-1).expand(text_mask.shape[0], text_mask.shape[1], mask.shape[-1]).float()\n",
    "        attn_mask = (attn_mask < 1)\n",
    "        s2s_attn.masked_fill_(attn_mask, 0.0)\n",
    "\n",
    "\n",
    "        mask_ST = mask_from_lens(s2s_attn, input_lengths, mel_input_length)\n",
    "        s2s_attn_mono = maximum_path(s2s_attn, mask_ST)\n",
    "        # s2s_attn = s2s_attn_mono\n",
    "\n",
    "        # encode\n",
    "        t_en = model.text_encoder(tokens, input_lengths, text_mask)\n",
    "        \n",
    "        asr = (t_en @ s2s_attn)\n",
    "\n",
    "        F0_real, _, F0 = model.pitch_extractor(mels.unsqueeze(1))\n",
    "        F0_real = F0_real.unsqueeze(0)\n",
    "        s = model.style_encoder(mels.unsqueeze(1))\n",
    "        real_norm = log_norm(mels.unsqueeze(1)).squeeze(1)\n",
    "        # out = model.decoder(asr, F0_real, real_norm, s)\n",
    "        out = model.decoder(asr, F0_real, real_norm, s)\n",
    "\n",
    "        # plot one s2s attention\n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.imshow(s2s_attn.squeeze().cpu().numpy(), aspect='auto', origin='lower')\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "    return out.squeeze().cpu().numpy()[..., :-50], s2s_attn, mels # weird pulse at the end of the model, need to be fixed later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29b0eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "\n",
    "# These lines are from the validation set\n",
    "lines = [\n",
    "    \"LibriTTS/train-clean-360/1392/140654/1392_140654_000034_000002.wav|lˈiːv ðə sˈɪnz ʌvðə bˈɑːdi, ænd wɪð ðˌaɪ bˈɑːdi pɹˈæktɪs vˈɜːtʃuː!|1030\",\n",
    "    \"LibriTTS/train-clean-360/14/208/14_208_000017_000000.wav|ɪt wʌz tˈuː dˈɜːɾi fɔːɹ mˈɪsɪz ˈælən tʊ ɐkˈʌmpəni hɜː hˈʌsbənd tə ðə pˈʌmp ɹˈuːm; hiː ɐkˈoːɹdɪŋli sˈɛt ˈɔf baɪ hɪmsˈɛlf, ænd kˈæθɹɪn hæd bˈɛɹli wˈɑːtʃt hˌɪm dˌaʊn ðə stɹˈiːt wɛn hɜː nˈoʊɾɪs wʌz klˈeɪmd baɪ ðɪ ɐpɹˈoʊtʃ ʌvðə sˈeɪm tˈuː ˈoʊpən kˈæɹɪdʒᵻz, kəntˈeɪnɪŋ ðə sˈeɪm θɹˈiː pˈiːpəl ðæt hæd sɚpɹˈaɪzd hɜː sˈoʊ mˌʌtʃ ɐ fjˈuː mˈɔːɹnɪŋz bˈæk.|1119\",\n",
    "    \"LibriTTS/train-clean-360/1401/146770/1401_146770_000031_000004.wav|hiː hɐdbɪn tə tʃˈɛdkuːm, ænd wʌz kˈʌmɪŋ bˈæk.|422\",\n",
    "    \"LibriTTS/train-clean-360/1401/174511/1401_174511_000039_000000.wav|hiː ˈʌɾɚd ɐ ɡɹˈaʊl ænd ðˈɛn θɹˈuː bˈæk hɪz kˈoʊt, dɪsplˈeɪɪŋ ɐ bˈædʒ ɐtˈætʃt tə hɪz vˈɛst.|422\",\n",
    "]\n",
    "\n",
    "wav_dir = '/data/LibriTTS/'\n",
    "silence = torch.zeros(24000 // 2).numpy()\n",
    "for i, lines in enumerate(lines):\n",
    "    wavs = []\n",
    "    wav_path, text, _ = lines.split('|')\n",
    "    ref_wav_path = os.path.join(wav_dir, wav_path)\n",
    "    # ref_wav_path = wav_path\n",
    "    print('Text:', text)\n",
    "    wav, s2s_attn, mels = inference(text, ref_wav_path)\n",
    "    ref, sr = librosa.load(ref_wav_path, sr=24000)\n",
    "    \n",
    "    wavs.append(ref)\n",
    "    wavs.append(silence)\n",
    "    wavs.append(wav)\n",
    "\n",
    "    res = np.concatenate(wavs, axis=0)\n",
    "\n",
    "    # Play the Reference, Silence, and Synthesized\n",
    "    print('Synthesized:', i)\n",
    "    display(ipd.Audio(res, rate=24000, normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67d40ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stts2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
